{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collaborated with Eric Cho to write this post. Given the a universe of N stocks with expected returns of μ and covariance matrix Σ, mean-variance portfolio problem can be defined as follows:\n",
    "\n",
    "$$\\underset{\\mathbf{w}}{\\text{min}} \\mathbf{w}'\\mathbf{\\Sigma}\\mathbf{w} $$\n",
    "where $w'\\mathbf{1}=1$ and $w'\\mu=z$. Here w is weights of the portfolio and z is the required return on the portfolio. This problem has a well-known solution.\n",
    "$$w=\\frac{C − zB}{AC − B^2}{\\Sigma}^{-1}\\mathbf{1}+\\frac{zA − B}{AC − B^2}{\\Sigma}^{-1}\\mu$$\n",
    "Despite the analytical solution to the problem, implementing optimal mean-variance portfolio in real-world is difficult because true expected returns and covariance matrix is hard to measure. I will explore using reinforcement learning to tackle the mean-variance portfolio problem.\n",
    "\n",
    "The reinforcement learning algorithm used in this post is Deep Q-Network (DQN).Instead of the mean-variance portfolio problem, I look at mean returns and variance / covariances among different stocks as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, I will use the RL_stock_prices.csv, which is uploaded in my repository. I randomly choose 3 stocks from Korea’s KOSPI index and look at daily prices 2010-01-01 ~ 2019-12-31. The three randomly picked stocks are 089470.KS - HDC Hyundai Engineering Plastic Co, 031440.KS - Shinsegae Food Co, and 012030.KS - DB Inc. The stock returns data can be found here. P1,P2, and P3 are the adjusted closing prices of 089470.KS, 031440.KS, and 012030.KS respectively. The stock price data is sourced from FN Guide. As the first step, I need to build the relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\minho\\Documents\\RL_stock_prices.csv\")\n",
    "df[['R1','R2','R3']] = df[['P1','P2','P3']]/df[['P1','P2','P3']].shift(1)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean-variance portfolio problem assumes that the stock return distribution is stationary and that $\\mu$ and $\\Sigma$ do not change. This is unlikely to be true in the real world where stock return distribution is likely to be time-varying. In order to capture the short-term to long-term changes in stock return distrbution, I look at 3 different windows in computing the past mean and variance/covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window1 = 20\n",
    "window2 = 50\n",
    "window3 = 100\n",
    "df[['M11','M12','M13']] = df[['R1','R2','R3']].rolling(window1).mean()\n",
    "df[['M21','M22','M23']] = df[['R1','R2','R3']].rolling(window2).mean()\n",
    "df[['M31','M32','M33']] = df[['R1','R2','R3']].rolling(window3).mean()\n",
    "for i in range(1,4):\n",
    "    for j in range(i,4):\n",
    "        df[f'V1{i}{j}'] = df[f'R{i}'].rolling(window1).cov(df[f'R{j}'])\n",
    "        df[f'V2{i}{j}'] = df[f'R{i}'].rolling(window2).cov(df[f'R{j}'])\n",
    "        df[f'V3{i}{j}'] = df[f'R{i}'].rolling(window3).cov(df[f'R{j}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the ModifiedTensorBoard class taken from pythonprogramming.net, which allows custom documentation of different training parameters across different epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        #self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overrided, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overrided\n",
    "    # We train for one batch only, n\n",
    "\n",
    "    # Overriding this method to stop creating default log writero need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overrided, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    def update_stats(self, **stats):\n",
    "        self._write_logs(stats, self.step)\n",
    "\n",
    "    def _write_logs(self, logs, index):\n",
    "        with self.writer.as_default():\n",
    "            for name, value in logs.items():\n",
    "                tf.summary.scalar(name, value, step=index)\n",
    "                self.step += 1\n",
    "                self.writer.flush()\n",
    "#    def update_stats(self, **stats):\n",
    "#        self._write_logs(stats, self.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the class function for DQNAgent. The core of DQN is Q-learning, which is a model-free reinforcement learning algorithm. It roots from state-action value function, also known as Q-function. The Q-function is defined as follows\n",
    "\n",
    "$$Q(s,a)=\\mathop{\\mathbb{E}}_{\\pi}[\\sum_{k=0}^\\infty \\gamma^{k}r_{t+k+1}|s_t=s,a_t=a]$$\n",
    "Intuitively, Q-function defines an intertemporal value of taking a policy π. If the policy is to always take the optimal action, the Q-function can be recursively written down as\n",
    "\n",
    "$$Q(s_t,a_t)=r(s_t,a_t),+\\gamma\\underset{a}{\\text{max}}Q(s_{t+1},a)$$\n",
    "The goal of Q-learning is to find a stable Q-function. This is done through value iteration method\n",
    "\n",
    "$$Q_{i+1}(s_t,a_t)=Q_i(s_t,a_t)+\\alpha(r(s_t,a_t)+\\gamma\\underset{a}{\\text{max}}Q^∗(s_{t+1},a)−Q_i(s_t,a_t))$$\n",
    "Here $Q_i$ is ith iteration estimation of Q-function and $Q^∗$ is the best estimate of Q-function. DQN uses deep learning to estimate the Q-function. To effectively train the deep learning model, DQN contains two main features\n",
    "\n",
    "1. Experience Replay\n",
    "2. Target Network\n",
    "\n",
    "Instead of fitting deep learning model on the entire past data, Experience Replay trains the model in random sample of past data, which reduces chances of overfitting and allows faster training. In my exercise, I used 10,000 replay memory size, 100 minimum replay memory size, and training size( minibatch size) of 64.\n",
    "\n",
    "Target Network is the concept of updating $Q^∗$ less frequently. By keeping $Q^∗$ more stable, $Q_i$ can be learned in a more stable manner. In my exercise, I update $Q^∗$ every 5 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, replay_memory_size = 10000, min_replay_memory_size = 100, minibatch_size=64, discount = 0.99, update_every = 5):\n",
    "        #Default Params\n",
    "        self.state_size = state_size\n",
    "        self.replay_memory_size = replay_memory_size\n",
    "        self.min_replay_memory_size = min_replay_memory_size\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.discount = discount\n",
    "        self.update_every = update_every\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.tensorboard = ModifiedTensorBoard(log_dir=\"logs/mean_variance\")\n",
    "\n",
    "        self.replay_memory = deque(maxlen=self.replay_memory_size)\n",
    "\n",
    "        self.target_update_counter = 0\n",
    "    \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=100, input_dim=self.state_size, activation=\"relu\"))\n",
    "        model.add(Dense(units=100, activation=\"relu\"))\n",
    "        model.add(Dense(units=100, activation=\"relu\"))\n",
    "        model.add(Dense(15, activation=\"linear\"))\n",
    "        model.compile(loss=huber_loss, optimizer=Adam(lr=0.001))\n",
    "        return model\n",
    "        \n",
    "    def act(self, state):\n",
    "        state = np.array(state)\n",
    "        predicted_qs = self.model.predict(state.reshape(-1,self.state_size))[0]\n",
    "        return np.argmax(predicted_qs)  \n",
    "        \n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self, terminal_state):\n",
    "        if len(self.replay_memory)<self.min_replay_memory_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.replay_memory, self.minibatch_size)\n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        \n",
    "        current_qs_list = self.target_model.predict(current_states)\n",
    "        \n",
    "        new_states = np.array([transition[3] for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_states)\n",
    "        \n",
    "        X,y = [],[]\n",
    "        \n",
    "        for index, (current_state, action, reward, new_state, done) in enumerate(minibatch):\n",
    "            max_future_q = np.max(future_qs_list[index])\n",
    "            if done:\n",
    "                new_q = reward\n",
    "            else:\n",
    "                new_q = reward + self.discount * max_future_q\n",
    "        \n",
    "            current_qs = current_qs_list[index]\n",
    "        \n",
    "            current_qs[action] = new_q\n",
    "        \n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "        \n",
    "        self.model.fit(np.array(X), np.array(y), batch_size = self.minibatch_size, verbose = 0, epochs = 0, shuffle = False, callbacks = None)\n",
    "        \n",
    "        \n",
    "        if terminal_state:\n",
    "            self.target_update_counter +=1\n",
    "        \n",
    "        if self.target_update_counter>self.update_every:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main design choices of DQN is the deep learning model architecture. While many studies in using Reinforcement Learning for trading uses more complicated form of networks such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), since the analytical solution to mean-variance portfolio problem is a simple function of $\\Sigma$ and $\\mu$, I focus on simple neural network consisting of dense layers as in create_model function. For the loss function, I decided to use Huber Loss function, which I found to be better at fitting in this setup. The code from loss function is derived from \n",
    "https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(a, b, in_keras=True):\n",
    "    error = a - b\n",
    "    quadratic_term = error*error / 2\n",
    "    linear_term = abs(error) - 1/2\n",
    "    use_linear_term = (abs(error) > 1.0)\n",
    "    if in_keras:\n",
    "        use_linear_term = K.cast(use_linear_term, 'float32') # Keras won't let us multiply floats by booleans, so we explicitly cast the booleans to floats\n",
    "    return use_linear_term * linear_term + (1-use_linear_term) * quadratic_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that DQNAgent class is done, instance of it can be created. As described previously, this exercise will use $\\mu$ and $\\Sigma$ estimated at different frequencies as featuers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = df.iloc[window3:].reset_index(drop = True)\n",
    "train_dt = dt.loc[dt['tradedate']<\"2019-01-01\"].reset_index(drop = True)\n",
    "test_dt = dt.loc[dt['tradedate']>=\"2019-01-01\"].reset_index(drop = True)\n",
    "\n",
    "features = [c for c in df.columns if c[0] in ['M','V']]\n",
    "\n",
    "agent = DQNAgent(len(features)+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is training the DQNAgent. One limitation of DQN algorithm is that the action space has to be discrete and finite. For this, I discretize the action space (weights to be placed on each stocks) into a space of 15 actions. Namely, I place a restriction that each stock can only have weights in the multiple of 25%.\n",
    "\n",
    "Although the original mean-variance portfolio problem attempts to solve minimization problem with the constraint that the mean return be equal to a pre-defined number, it isn’t clear how this constraint can be enforced in DQN context. Even in Ledoit, Wolf (2003), an influential work on empirically estimating $\\Sigma$ through shrinkage, mean return is ignored when testing for out of sample performance. This exercise enforces reward function based on portfolio variance.\n",
    "\n",
    "To prevent overfitting, I randomly pick a sub-period of 1000 trading days on each episode of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|######################################################################| 1000/1000 [27:16:54<00:00, 98.21s/episode]\n"
     ]
    }
   ],
   "source": [
    "#Training Variables\n",
    "epsilon = 0.8\n",
    "N_EPISODES = 1000\n",
    "MIN_EPSILON = 0.05\n",
    "EPSILON_DECAY = 0.995\n",
    "EPISODE_LENGTH = 1000\n",
    "AGGREGATE_STATS_EVERY=5 #Update tensorboard every 5 episodes\n",
    "\n",
    "weight_map = [[0,0,4],[1,0,3],[2,0,2],\n",
    "[3,0,1],[4,0,0],[0,1,3],[1,1,2],[2,1,1],\n",
    "[3,1,0],[0,2,2],[1,2,1],[2,2,0],[0,3,1],\n",
    "[1,3,0],[0,4,0]]\n",
    "\n",
    "\n",
    "ep_rewards = [] #History of episode rewards\n",
    "\n",
    "for episode in tqdm(range(1, N_EPISODES + 1), ascii=True, unit = \"episode\"):\n",
    "    agent.tensorboard.step = episode\n",
    "    episode_st_idx = np.random.randint(len(train_dt) - EPISODE_LENGTH)\n",
    "    episode_dt = train_dt.iloc[episode_st_idx:episode_st_idx+EPISODE_LENGTH].reset_index(drop=True)\n",
    "    \n",
    "    ret_history = [] #History of portfolio returns\n",
    "    \n",
    "    current_state = [0]*(len(features)+2) #initialize state variable with 0s\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for idx,d in episode_dt.iterrows():\n",
    "        #Epsilon greedy algorithm. Choose random action if random number less than epsilon \n",
    "        if np.random.random()<=epsilon:\n",
    "            action = np.random.randint(0,15)\n",
    "        else:\n",
    "            action =  agent.act(current_state)\n",
    "        \n",
    "        #Map action to portfolio weights\n",
    "        weights = np.array(weight_map[action])/4\n",
    "                \n",
    "        #Process action\n",
    "        ret_history.append(sum(weights*d[['R1','R2','R3']]))\n",
    "        reward = 0\n",
    "\n",
    "        if idx == EPISODE_LENGTH-1:\n",
    "            done = True\n",
    "            reward = -np.var(ret_history)\n",
    "        else:\n",
    "            done = False\n",
    "        episode_reward += reward\n",
    "        new_state = [np.var(ret_history), idx/EPISODE_LENGTH] + list(d[features])\n",
    "        \n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done)\n",
    "        \n",
    "        #Update current state with new state        \n",
    "        current_state = new_state \n",
    "        \n",
    "    #Update Tensorboard\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
    "    \n",
    "    #Decay epsilon every episode\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "\n",
    "agent.model.save_weights(\"trained_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try deploying the model to out of sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio Variance: 0.0002162661464555882\n",
      "S1 Variance: 0.00039278878062088676\n",
      "S2 Variance: 0.0002475167239713626\n",
      "S3 Variance: 0.00031164781447791775\n"
     ]
    }
   ],
   "source": [
    "#load Trained Model\n",
    "agent.model.load_weights(\"trained_model\")\n",
    "\n",
    "#initialize variables\n",
    "current_state = [0]*(len(features)+2)\n",
    "weight_history = []\n",
    "ret_history = []\n",
    "\n",
    "EPISODE_LENGTH = len(test_dt)\n",
    "for idx,d in test_dt.iterrows():    \n",
    "    action =  agent.act(current_state)\n",
    "\n",
    "    #Map action to portfolio weights\n",
    "    weights = np.array(weight_map[action])/4\n",
    "    weight_history.append(weights)\n",
    "\n",
    "    #Process action\n",
    "    ret_history.append(sum(weights*d[['R1','R2','R3']]))\n",
    "    reward = 0\n",
    "\n",
    "    if idx == EPISODE_LENGTH-1:\n",
    "        done = True\n",
    "        reward = -np.var(ret_history)\n",
    "    else:\n",
    "        done = False\n",
    "    episode_reward += reward\n",
    "    new_state = [np.var(ret_history), idx/EPISODE_LENGTH] + list(d[features])\n",
    "    \n",
    "    current_state = new_state\n",
    "print(\"Portfolio Variance:\", -reward)\n",
    "print(\"S1 Variance:\", np.var(test_dt['R1']))\n",
    "print(\"S2 Variance:\", np.var(test_dt['R2']))\n",
    "print(\"S3 Variance:\", np.var(test_dt['R3']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears portfolio variance by DQN agent has lower variance than any of the singular stock variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWWElEQVR4nO3dfZBcVZnH8e8z3T15MYG8SjCTkOgmS2IVQXYctRDM1pYacDULtSrorhqFSGFYV4uVCCXgUkvJbkGxFggJEREKCVi8BTZrlqJ2lyot1wy7kBDy4oQIGYIwBMRAJum3Z//o25OepmemJ7lNzz39+1RNpe+9Z+59TnX4cXL63tPm7oiISPK1NbsAERGJhwJdRCQQCnQRkUAo0EVEAqFAFxEJRLpZF54xY4bPmzevWZcXEUmkJ5988lV3n1nrWNMCfd68eXR3dzfr8iIiiWRmzw91TFMuIiKBUKCLiARCgS4iEggFuohIIBToIiKBUKCLiARixEA3s9vN7BUze2aI42ZmPzSzHjPbYmanxV+miIiMpJ4R+h3AsmGOnwUsiH5WArcce1kiIjJaIz5Y5O5PmNm8YZosB+700sLqvzazKWZ2oru/FFeRla77zXVs6dtCf76/EacXEWm4k6edzLVnXBv7eeN4UnQ2sLdiuzfa97ZAN7OVlEbxzJ0796gvuO+tfezv34+ZHfU5RESawqHvYF9DTh1HoNdK1Zpfg+Tua4G1AJ2dnUf1VUmXdV3Gm7k3eajnIfRtSyKSRFPGT2nIeeO4y6UXmFOx3QHsi+G8IiIyCnEE+gbgS9HdLh8G3mjU/LmIiAxtxCkXM7sHWArMMLNe4CogA+DutwIbgbOBHuAgsKJRxYqIyNDqucvl/BGOO/CN2CoSEZGjoidFRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQNQV6Ga2zMx2mlmPma2ucfx4M3vEzJ42s21mtiL+UkVEZDgjBrqZpYCbgbOAxcD5Zra4qtk3gGfdfQmwFLjezNpjrlVERIZRzwi9C+hx9+fcPQusB5ZXtXFgspkZMAl4DcjHWqmIiAyrnkCfDeyt2O6N9lW6CVgE7AO2At9092L1icxspZl1m1l3X1/fUZYsIiK11BPoVmOfV21/EngKeA9wKnCTmR33tl9yX+vune7eOXPmzFGWKiIiw6kn0HuBORXbHZRG4pVWAA94SQ+wBzg5nhJFRKQe9QT6ZmCBmc2PPug8D9hQ1eYF4C8AzOwE4E+B5+IsVEREhpceqYG7581sFbAJSAG3u/s2M7soOn4rcA1wh5ltpTRFc5m7v9rAukVEpMqIgQ7g7huBjVX7bq14vQ/4RLyliYjIaOhJURGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQmEAl1EJBAKdBGRQCjQRUQCoUAXEQlEutkFiIg0wuTUZC6ceyFzJszBsGaXM0i6Lc327duHbTN+/Hg6OjrIZDL1n/dYCxMRGYsunHshSzqW0D65HbOxFejtqXYWTF0w5HF3Z//+/fT29jJ//vy6z6spFxEJ0pwJc8ZkmNfDzJg+fTqHDh0a1e8p0EUkSIYlMszLjqZ2BbqISCDqmkM3s2XAvwIpYJ27/6BGm6XAjUAGeNXdPxZblSIix+iLP3qBPxwsxna+KRPbuPviuSO2W3PDGjY+sJG2VBtt1sZV11/F9qe387O1P2P37t309fUxY8aMWGoaMdDNLAXcDHwc6AU2m9kGd3+2os0U4EfAMnd/wczeHUt1IiIxiTPM6z3fU5uf4onHnuDnj/+c9nHtvL7/dXK5HBPHT+Srn/sqS5cujbWmekboXUCPuz8HYGbrgeXAsxVtvgA84O4vALj7K7FWKSKSQH0v9zFl2hTax7UDMHX6VAA6Zncwb+q82K9Xzxz6bGBvxXZvtK/SQmCqmf2XmT1pZl+Kq0ARkaQ6fenp/P7F3/OpD32Ka75zDZt/ubmh16sn0Gt91OpV22ngz4BPAZ8EvmdmC992IrOVZtZtZt19fX2jLlZEJEkmTprIfY/fx1U3XMXU6VO59MJLeeiehxp2vXqmXHqBORXbHcC+Gm1edfe3gLfM7AlgCbCrspG7rwXWAnR2dlb/T0FEJDipVIqu07voOr2LhYsW8vC9D/O5v/lcQ65Vzwh9M7DAzOabWTtwHrChqs3DwBlmljazicCHgOGfaxURCdyenj08v/v5ge0dz+zgPR3vadj1Rgx0d88Dq4BNlEL6PnffZmYXmdlFUZvtwC+ALcBvKN3a+EzDqhYRGaUpE+N97Kae8x186yCXX3I5nzn9M5zzsXPYvWs3F3/nYu5ccycdHR309vZyyimncMEFF8RSk7k3Z+ajs7PTu7u7j+p3v/fL7/FQz0PxFiQiQblx8Y3Mmj+r2WXUNNJaLmXbt29n0aJFg/aZ2ZPu3lmrvZ4UFREJhAJdRCQQCnQRkUAo0EVEAqFAFxEJhAJdRCQQ+go6EWkJC3/8l2T6X4vtfLkJ09j1tUdHbFdr+dx71t3Dzi07yWQydHV1sWbNmlF9d+hQNEIXkZYQZ5jXe77K5XMf/O8HWXf/OmbNnsWnP/tpduzYwdatW+nv72fdunWx1KQRuohIgwy3fG75K+a6urro7e2N5XoaoYuINMhIy+fmcjnuuusuli1bFsv1FOgiIg0y0vK5F198MWeeeSZnnHFGLNfTlIuISAMNtXzu97//ffr6+lizZk1s11Kgi4g0yJ6ePbRZGye97yTgyPK59915H5s2beLxxx+nrS2+iRIFuoi0hNyEabHftjiSg28d5NrvXsuBNw6QSqeYO38uV19/NUvfv5STTjqJj3zkIwCce+65XHnllcdcUxCBPimX4dxXjgPgwXcf4EAm2+SKRGSsqeee8bi9f8n7uXvj3W/bv71ve13L545WEIH+0deP4x+y/wfAa/tP49FZrza5IhGRd14Qd7mMKx75HusJTfrCDhGRZgsi0DMVIZ5RnotIiwoi0Nu99msRkVYSRKBXjsozmnIRkRYVRqBXvlaei0iLCuIul4w7BTfypEijRBeRt/v6Y1/njewbsZ3v+PbjWfPxkZ/yrLV87gN3PcCurbtwdxYuXMgdd9zBpEmTjrmmYAI9R5o8bWQU6CJSQ5xhXu/5KpfPbR/Xzuv7XyeXy3H5P13OB076AADf/va3uemmm1i9evUx1xRMoGdJU6RNc+giMmYMtXxue6q07e709/cPLKV7rAKZQy8Fepa0Al1Exozhls9dsWIFs2bNYseOHVxyySWxXC+MQHcnR4ocKTIUm12OiAgw/PK5P/nJT9i3bx+LFi3i3nvvjeV6QQR6O8VSoHuatEboIjKGlJfPXXXZKq74wRU89uhjg459/vOf5/7774/lWkEEesajQCetEbqIjBl7evbw/O7nB7Z3PLODE2efyPPPlfa5O4888ggnn3xyLNcL40PRaIRe1JSLiAzh+PbjY79tcSS1ls+98l+u5Ftf+RbZg1ncnSVLlnDLLbfEUlMYge5FctZGwTNkXIEuIm9Xzz3jcRtq+dz1v1jfkOVzw5hyoUhWUy4i0uKCCfQcKQ57hgyFZpcjItIUgQR6gSxtZMnQrhG6iACO4wm+6+1oaq8r0M1smZntNLMeMxvy+VQz+6CZFczsr0ddyTFo9yJ52kpTLq4RuojA3v69ZA9kExnq7s7+/fsZP378qH5vxA9FzSwF3Ax8HOgFNpvZBnd/tka764BNo6ogBhkK5CwKdAoMXn9RRFrRbS/cxoVcyJwJczDiebQ+Lum2NPmJ+WHbjB8/no6OjtGdt442XUCPuz8HYGbrgeXAs1XtLgHuBz44qgpi0E6BPFZ69F9TLiICHCgc4IY9NzS7jJrmHTePR855JPbz1jPlMhvYW7HdG+0bYGazgXOAW4c7kZmtNLNuM+vu6+sbba1DGhihe5p2hv+/nohIqOoJ9Fr/VqmelLoRuMx9+Alsd1/r7p3u3jlz5sw6SxxZhgI5LFrLRXPoItKa6ply6QXmVGx3APuq2nQC66MlIGcAZ5tZ3t0fiqPIkWQoDHwoqhG6iLSqegJ9M7DAzOYDLwLnAV+obODu88uvzewO4NF3KswBMuQpGBxG96GLSOsaMdDdPW9mqyjdvZICbnf3bWZ2UXR82Hnzd0L5Q9HSXS4aoYtIa6prLRd33whsrNpXM8jd/SvHXlb9vOiMsxwFs9LyuVbEiuBBPDIlIlK/xMdeOupCAchF/39KK81FpAUlPvkyxSOBniU1aJ+ISCtJfPJlvBTiRTsyQs8UU80sSUSkKRIf6OloNF6kItA15SIiLSjxyZcplp57cvOKEfrYWrdBROSdkPxAHxiNO1nPVO0TEWkdiU++gQ9AzcmW73LRCF1EWlDyA92j8HYnV77LRSN0EWlBiU++8nx5W8UcertG6CLSghIf6OlohG4UKx4sUqCLSOtJfKCXp1dSFMl6+bbFZlYkItIcAQT6kdcDUy4aoYtIC0p+oEfz5cVotUWAcfoWOhFpQckP9GiEXoy+U7S0TyN0EWk9AQT6kRH6kUBvZkUiIs2R+EBvj8K7QOlLoiv3iYi0ksQHeroc6J6qWJyriQWJiDRJ4gO9HN55FOgi0toCCvS05tBFpKUFEOil9M4V0xVPiirRRaT1JD/QiQLdx1EgRcFtIORFRFpJ4gO93SHrKQ7beKD0tGg55EVEWkniAz3tpVUWc14R6Bqhi0gLSnygt3uRHCkODwR6SiN0EWlJ6WYXcDS8WOTK5ydzQvEQf+J7yZHmcPHICH1pvpfZe6Y3uUoRkdp+eXxjzpvIQM/8Mctni9t4wafze5tMd3oafYdPAODfCh/k1PQOJpFtcpUiIrW96+AbDTlvIgPdiqUplQfHzea2AxeTP5TmEBMB+Mf8CsbnD5K2fDNLFBEZ0qxJ0/m7Bpw3kYHeViitj2s4b/pxbzt+iIloGl1ExqqitTfkvIn8ULStUE5rpbaISFkiA90qRugiIlKSyEAvT7m0mb6aSESkLJGBnioHOgp0EZGyugLdzJaZ2U4z6zGz1TWOf9HMtkQ/vzKzJfGXekRbMZpycQW6iEjZiIFuZingZuAsYDFwvpktrmq2B/iYu58CXAOsjbvQSuVAd9N3h4qIlNUzQu8Cetz9OXfPAuuB5ZUN3P1X7v56tPlroCPeMgcrz6EXUaCLiJTVE+izgb0V273RvqF8Dfj3WgfMbKWZdZtZd19fX/1VVimP0IueyI8AREQaop5ErDUMrnm/oJn9OaVAv6zWcXdf6+6d7t45c+bM+quskooCvZDMz3RFRBqinidFe4E5FdsdwL7qRmZ2CrAOOMvd98dTXm1txQIARVOgi4iU1ZOIm4EFZjbfzNqB84ANlQ3MbC7wAPC37r4r/jIHK4/Q855q9KVERBJjxBG6u+fNbBWwCUgBt7v7NjO7KDp+K3AlMB34kZXuPMm7e2ejih6YclGgi4gMqGtxLnffCGys2ndrxesLgAviLW1oqWjKJZfMtcVERBoikZPQ5UDPemNWLBMRSaJkBroXKbop0EVEKiQy0NPFQumLoZnQ7FJERMaMRAZ6mxc5TJps9MXQIiKS0EBPe4EcKQ4p0EVEBiQz0IulQM/6uGaXIiIyZiQz0L00h97vmkMXESlLZKCnylMuxXc1uxQRkTEjkYFeGqG3cdAnNrsUEZExI7GBnrUU/UUFuohIWUIDPU+WFG9phC4iMiChgV6acul3zaGLiJQlOtBz6NF/EZGyRAZ6hjw5tHSuiEilZAa658mbviBaRKRSIgM9TWnKRUREjkhkKmY8T07fJyoiMkgiUzFDgTyachERqZTQQM8r0EVEqiQ00Av6UFREpEpCA10jdBGRaokLdC8WaSdPQSN0EZFBEhfohUKeNnOKzS5ERGSMSVyg57KHASg0uQ4RkbEmcYGeLQe6ZlxERAZJXKDns4cANOUiIlIleYGeK43QMW9uISIiY0zyAj2bBUBxLiIyWAIDvb/0QiN0EZFBkhfoudIIHVegi4hUSlygF6I5dDN9LCoiUilxgV6MAr1N97mIiAySuEDP50uBntLHoiIig9QV6Ga2zMx2mlmPma2ucdzM7IfR8S1mdlr8pZYUozl01xy6iMggIwa6maWAm4GzgMXA+Wa2uKrZWcCC6GclcEvMdQ4o5qNA1+JcIiKD1DNC7wJ63P05d88C64HlVW2WA3d6ya+BKWZ2Ysy1AlCMplyKWj5XRGSQdB1tZgN7K7Z7gQ/V0WY28FJlIzNbSWkED/Cmme0cVbWRKcenTx03cYe9ll+VO5rfT7Ji/4FU24TJLbc2mfrdWkLv94vZ/j/apS+/UOPQDODVEX79pKEO1BPotYbC1RPY9bTB3dcCa+u45vAFmXX7H97oPNbzJJGZdecPvNpyfVe/W0sr99vdj7rf9Uy59AJzKrY7gH1H0UZERBqonkDfDCwws/lm1g6cB2yoarMB+FJ0t8uHgTfc/aXqE4mISOOMOOXi7nkzWwVsAlLA7e6+zcwuio7fCmwEzgZ6gIPAisaVDMQwbZNgrdp39bu1qN9HwXQ/t4hIGBL3pKiIiNSmQBcRCUTiAn2kZQhCYma/M7OtZvaUmXVH+6aZ2WNm9tvoz6nNrvNYmdntZvaKmT1TsW/IfprZd6P3f6eZfbI5VR+7Ifp9tZm9GL3nT5nZ2RXHQun3HDP7TzPbbmbbzOyb0f6g3/Nh+h3fe+7uifmh9KHsbuC9QDvwNLC42XU1sL+/A2ZU7ftnYHX0ejVwXbPrjKGfZwKnAc+M1E9Ky088DYwD5kd/H1LN7kOM/b4auLRG25D6fSJwWvR6MrAr6l/Q7/kw/Y7tPU/aCL2eZQhCtxz4afT6p8BfNa+UeLj7E8BrVbuH6udyYL27H3b3PZTurOp6J+qM2xD9HkpI/X7J3f83en0A2E7pyfKg3/Nh+j2UUfc7aYE+1BIDoXLgP8zsyWjZBIATPLrHP/rz3U2rrrGG6mcr/B1YFa1aenvFtEOQ/TazecAHgP+hhd7zqn5DTO950gK9riUGAnK6u59GaTXLb5jZmc0uaAwI/e/ALcD7gFMprYV0fbQ/uH6b2STgfuDv3f2PwzWtsS+xfa/R79je86QFekstMeDu+6I/XwEepPTPrZfLK1lGf77SvAobaqh+Bv13wN1fdveCuxeB2zjyT+yg+m1mGUqhdre7PxDtDv49r9XvON/zpAV6PcsQBMHM3mVmk8uvgU8Az1Dq75ejZl8GHm5OhQ03VD83AOeZ2Tgzm09pDf7fNKG+hqhadvocSu85BNRvMzPgx8B2d7+h4lDQ7/lQ/Y71PW/2J79H8Unx2ZQ+Hd4NXNHsehrYz/dS+oT7aWBbua/AdOBx4LfRn9OaXWsMfb2H0j81c5RGJV8brp/AFdH7vxM4q9n1x9zvu4CtwJboP+gTA+z3RylNHWwBnop+zg79PR+m37G953r0X0QkEEmbchERkSEo0EVEAqFAFxEJhAJdRCQQCnQRkUAo0EVEAqFAFxEJxP8D1gBwKakEIKcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weight_df = pd.DataFrame(weight_history, columns = [\"S1\",\"S2\",\"S3\"])\n",
    "weight_df.plot.area()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at weights chosen by the DQN agent, 50%, 25%, 25% is chosen for each stocks. This is close to the portfolio weights using shrinkage estimator when the lagrangian to the mean return is approximately 2. \n",
    "$$\\gamma=\\frac{zA−B}{AC−B^2}⟹z=\\frac{\\gamma(AC−B^2)+B}{A}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55581637 0.24408406 0.20009957]\n"
     ]
    }
   ],
   "source": [
    "mu = train_dt[['R1','R2','R3']].apply(lambda x: np.mean(x), axis = 0)\n",
    "Sigma = LedoitWolf().fit(train_dt[['R1','R2','R3']]).covariance_\n",
    "Sigma_inv = np.linalg.inv(Sigma)\n",
    "\n",
    "A = np.matmul(np.matmul(np.transpose(np.ones(3)), Sigma_inv), np.ones(3))\n",
    "B = np.matmul(np.matmul(np.transpose(mu), Sigma_inv), np.ones(3))\n",
    "C = np.matmul(np.matmul(np.transpose(mu), Sigma_inv), mu)\n",
    "\n",
    "gamma = 2\n",
    "z = (gamma*(A*C-B**2)+B)/A\n",
    "\n",
    "optimal_weight = (C - z*B)/(A*C-B**2)*np.matmul(Sigma_inv,np.ones(3)) + (z*A - B)/(A*C-B**2)*np.matmul(Sigma_inv, mu)\n",
    "print(optimal_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
